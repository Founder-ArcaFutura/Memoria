name: Evaluation suites

on:
  workflow_dispatch:
    inputs:
      suites:
        description: "Optional comma or space separated suite identifiers to evaluate"
        required: false
      spec:
        description: "Optional path to an evaluation spec override relative to the repo root"
        required: false
      dataset_root:
        description: "Optional directory with fixture overrides (relative path or absolute)"
        required: false
      post_pr_comment:
        description: "Set to true to publish the evaluation summary as a PR comment on manual runs"
        required: false
        type: boolean
        default: false
  pull_request:
    branches: [ main ]
    paths:
      - "memoria/evaluation/**"
      - "memoria/policy/**"
      - "memoria/retrieval/**"
      - "memoria/tools/benchmark.py"
      - "scripts/ci/run_evaluation_suites.py"
      - "scripts/ci/run_policy_telemetry_snapshot.py"
      - "docs/configuration/evaluation-suites.md"

jobs:
  run-benchmarks:
    name: Run evaluation smoke suites
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      MEMORIA_EVALUATION_SPEC_PATH: ${{ github.event.inputs.spec || '' }}
      MEMORIA_EVALUATION_DATASET_ROOT: ${{ github.event.inputs.dataset_root || '' }}
      MEMORIA_EVALUATION_SUITES: ${{ github.event.inputs.suites || '' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: |
            pyproject.toml
            requirements.txt

      - name: Install evaluation dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ".[benchmark]"

      - name: Extract dataset overrides
        if: ${{ secrets.MEMORIA_EVALUATION_DATASET_ARCHIVE != '' && env.MEMORIA_EVALUATION_DATASET_ROOT == '' }}
        env:
          ARCHIVE_PAYLOAD: ${{ secrets.MEMORIA_EVALUATION_DATASET_ARCHIVE }}
        run: |
          mkdir -p evaluation-datasets
          python - <<'PY'
import base64
import os
from pathlib import Path
import tarfile

payload = os.environ.get("ARCHIVE_PAYLOAD", "").strip()
if not payload:
    raise SystemExit(0)
archive_dir = Path("evaluation-datasets")
archive_dir.mkdir(parents=True, exist_ok=True)
archive_path = archive_dir / "datasets.tar.gz"
archive_path.write_bytes(base64.b64decode(payload))
with tarfile.open(archive_path, "r:gz") as handle:
    handle.extractall(archive_dir)
PY
          echo "MEMORIA_EVALUATION_DATASET_ROOT=$(pwd)/evaluation-datasets" >> "$GITHUB_ENV"

      - name: Run benchmark harness
        run: |
          python scripts/ci/run_evaluation_suites.py \
            --output-dir evaluation-artifacts \
            --min-precision 0.60 \
            --min-recall 0.60 \
            --max-cost-per-query 0.40 \
            --max-latency-p95 2500

      - name: Capture policy telemetry snapshot
        run: |
          python scripts/ci/run_policy_telemetry_snapshot.py

      - name: Upload policy telemetry snapshot
        uses: actions/upload-artifact@v4
        with:
          name: policy-telemetry
          path: telemetry-artifacts
          if-no-files-found: error

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: evaluation-artifacts
          if-no-files-found: error

      - name: Determine comment preference
        id: comment-preference
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "post=true" >> "$GITHUB_OUTPUT"
          elif [ "${{ github.event.inputs.post_pr_comment }}" = "true" ]; then
            echo "post=true" >> "$GITHUB_OUTPUT"
          else
            echo "post=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Post evaluation summary comment
        if: ${{ steps.comment-preference.outputs.post == 'true' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            const core = require('@actions/core');

            const commentPath = path.join(process.cwd(), 'evaluation-artifacts', 'pull_request_comment.md');
            if (!fs.existsSync(commentPath)) {
              core.info('No PR summary found, skipping comment.');
              return;
            }

            if (!context.payload.pull_request || !context.issue.number) {
              core.info('Pull request context missing; skipping comment publication.');
              return;
            }

            const body = fs.readFileSync(commentPath, 'utf8').trim();
            if (!body) {
              core.info('PR summary file is empty, skipping comment.');
              return;
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });
